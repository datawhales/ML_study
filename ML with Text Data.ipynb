{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "looking-capture",
   "metadata": {},
   "source": [
    "## nltk 모듈의 sent_tokenize\n",
    "입력 텍스트를 문장 단위로 토큰화해준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "efficient-dialogue",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/datawhales/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk import sent_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "text_sample = 'The Matrix is everywhere its all around us, here even in this room. \\\n",
    "               You can see it out your window or on your television. \\\n",
    "               You feel it when you go to work, or go to church or pay your taxes.'\n",
    "sentences = sent_tokenize(text=text_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "democratic-silver",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The Matrix is everywhere its all around us, here even in this room.',\n",
       " 'You can see it out your window or on your television.',\n",
       " 'You feel it when you go to work, or go to church or pay your taxes.']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dangerous-tiger",
   "metadata": {},
   "source": [
    "## nltk 모듈의 word_tokenize\n",
    "입력 텍스트를 단어 단위로 토큰화해준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "native-quilt",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'Matrix', 'is', 'everywhere', 'its', 'all', 'around', 'us', ',', 'here', 'even', 'in', 'this', 'room', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "\n",
    "sentence = 'The Matrix is everywhere its all around us, here even in this room.'\n",
    "words = word_tokenize(sentence)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "north-audience",
   "metadata": {},
   "source": [
    "## 여러 개 문장으로 이루어진 텍스트 데이터를 문장별로 단어 토큰화하는 함수 작성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "rising-communications",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import sent_tokenize, word_tokenize\n",
    "\n",
    "def tokenize_text(text):\n",
    "    \"\"\" 주어진 텍스트를 먼저 문장 단위로 토큰화하고,\n",
    "        각 문장을 단어 단위로 토큰화하는 함수.\n",
    "    \"\"\"\n",
    "    sentences = sent_tokenize(text)\n",
    "    word_tokens = [word_tokenize(sentence) for sentence in sentences]\n",
    "    return word_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "brilliant-diameter",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['The', 'Matrix', 'is', 'everywhere', 'its', 'all', 'around', 'us', ',', 'here', 'even', 'in', 'this', 'room', '.'], ['You', 'can', 'see', 'it', 'out', 'your', 'window', 'or', 'on', 'your', 'television', '.'], ['You', 'feel', 'it', 'when', 'you', 'go', 'to', 'work', ',', 'or', 'go', 'to', 'church', 'or', 'pay', 'your', 'taxes', '.']]\n"
     ]
    }
   ],
   "source": [
    "word_tokens = tokenize_text(text_sample)\n",
    "print(word_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nominated-effectiveness",
   "metadata": {},
   "source": [
    "## stopwords\n",
    "stopwords란 텍스트 분석에 있어서 큰 의미가 없는 단어를 말한다. 문법적인 특성으로 인해 텍스트에 자주 나타나지만 문장을 이해하는데 중요한 특성으로 사용되지 않는 단어들이 해당된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "operating-rouge",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 stopwords 개수: 179\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/datawhales/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "print(f\"영어 stopwords 개수: {len(nltk.corpus.stopwords.words('english'))}\")\n",
    "print(nltk.corpus.stopwords.words('english')[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "liable-counter",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['matrix', 'everywhere', 'around', 'us', ',', 'even', 'room', '.'], ['see', 'window', 'television', '.'], ['feel', 'go', 'work', ',', 'go', 'church', 'pay', 'taxes', '.']]\n"
     ]
    }
   ],
   "source": [
    "# 단어 단위로 토큰화된 text_sample인 word_tokens에서 stopwords 제거\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "tokens = []\n",
    "for sent in word_tokens:\n",
    "    filtered_words = [word.lower() for word in sent if word.lower() not in stopwords]\n",
    "    tokens.append(filtered_words)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "color-sunday",
   "metadata": {},
   "source": [
    "## Stemming & Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "compliant-prize",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "work work work\n",
      "amus amus amus\n",
      "happy happiest\n",
      "fant faciest\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "stemmer = LancasterStemmer()\n",
    "\n",
    "print(stemmer.stem('working'), stemmer.stem('works'), stemmer.stem('worked'))\n",
    "print(stemmer.stem('amusing'), stemmer.stem('amuses'), stemmer.stem('amused'))\n",
    "print(stemmer.stem('happier'), stemmer.stem('happiest'))\n",
    "print(stemmer.stem('fancier'), stemmer.stem('faciest'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "young-ghost",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/datawhales/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amuse amuse amuse\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "\n",
    "lemma = WordNetLemmatizer()\n",
    "print(lemma.lemmatize('amusing', 'v'), lemma.lemmatize('amuses', 'v'), lemma.lemmatize('amused', 'v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cardiac-nutrition",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "happy happy\n",
      "fancy fancy\n"
     ]
    }
   ],
   "source": [
    "print(lemma.lemmatize('happier', 'a'), lemma.lemmatize('happiest', 'a'))\n",
    "print(lemma.lemmatize('fancier', 'a'), lemma.lemmatize('fanciest', 'a'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "superb-greensboro",
   "metadata": {},
   "source": [
    "## Bag of Words - BOW\n",
    "Bag of Words 모델은 문서가 가지는 모든 단어를 문맥이나 순서를 무시하고 각 단어에 대해 빈도(frequency) 값을 부여하여 피처 값을 추출하는 모델이다.  \n",
    "BOW 모델의 단점으로는 semantic context(문맥적 의미)를 반영하기 어렵다는 것과 sparse matrix 문제가 존재한다.  \n",
    "sparse matrix 문제란 문서마다 서로 다른 단어로 구성되기 때문에 하나의 문서에 있는 단어가 전체 단어 종류 중 극히 일부분이 되므로 대부분의 데이터가 0으로 채워지는 것을 말한다. 이러한 sparse matrix는 ML 알고리즘의 예측 성능을 떨어뜨리게 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moderate-mining",
   "metadata": {},
   "source": [
    "BOW 모델에서의 feature vectorization.  \n",
    "text 데이터를 수치화해야 하는데, 모든 문서에서 모든 단어를 column 형태로 나열하고 각 문서에서 해당 단어의 횟수를 값으로 부여하여 M * N 형태의 matrix를 생성한다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "traditional-engineer",
   "metadata": {},
   "source": [
    "## TF-IDF\n",
    "TF-IDF란 Term Frequency Inverse Document Frequency의 약자로, 개별 문서에서 자주 나타나는 단어일수록 높은 가중치를 주되, 모든 문서에서 전반적으로 자주 나타나는 단어에 대해서는 패털티를 주는 방식으로 값을 부여하는 방식을 말한다.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "congressional-estate",
   "metadata": {},
   "source": [
    "## Sparse Matrix의 처리 - COO, CSR 방식\n",
    "COO(Coordinate) 방식은 0이 아닌 데이터만 별도의 데이터 배열에 저장하고 그 데이터가 가리키는 행과 열의 위치를 별도의 배열로 저장하는 방식.  \n",
    "CSR(Compressed Sparse Row) 방식은 COO 방식이 위치를 표현할 때 같은 데이터를 반복적으로 사용하는 것을 해결한 방식.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "stylish-elizabeth",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COO\n",
    "import numpy as np\n",
    "\n",
    "dense = np.array([[3,0,1], [0,2,0]])\n",
    "\n",
    "from scipy import sparse\n",
    "\n",
    "data = np.array([3, 1, 2])\n",
    "\n",
    "# (0, 0)에 3, (0, 2)에 1, (1, 1)에 2\n",
    "row_pos = np.array([0, 0, 1])\n",
    "col_pos = np.array([0, 2, 1])\n",
    "\n",
    "sparse_coo = sparse.coo_matrix((data, (row_pos, col_pos)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cubic-minutes",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3, 0, 1],\n",
       "       [0, 2, 0]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparse_coo.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "least-tribe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1 0 0 5]\n",
      " [1 4 0 3 2 5]\n",
      " [0 6 0 3 0 0]\n",
      " [2 0 0 0 0 0]\n",
      " [0 0 0 7 0 8]\n",
      " [1 0 0 0 0 0]]\n",
      "--------------------\n",
      "[[0 0 1 0 0 5]\n",
      " [1 4 0 3 2 5]\n",
      " [0 6 0 3 0 0]\n",
      " [2 0 0 0 0 0]\n",
      " [0 0 0 7 0 8]\n",
      " [1 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# CSR\n",
    "from scipy import sparse\n",
    "dense2 = np.array([[0,0,1,0,0,5],\n",
    "                   [1,4,0,3,2,5],\n",
    "                   [0,6,0,3,0,0],\n",
    "                   [2,0,0,0,0,0],\n",
    "                   [0,0,0,7,0,8],\n",
    "                   [1,0,0,0,0,0]])\n",
    "\n",
    "# 0이 아닌 데이터\n",
    "data2 = np.array([1, 5, 1, 4, 3, 2, 5, 6, 3, 2, 7, 8, 1])\n",
    "\n",
    "row_pos = np.array([0,0,1,1,1,1,1,2,2,3,4,4,5])\n",
    "col_pos = np.array([2,5,0,1,3,4,5,1,3,0,3,5,0])\n",
    "\n",
    "sparse_coo = sparse.coo_matrix((data2, (row_pos, col_pos)))\n",
    "\n",
    "row_pos_ind = np.array([0,2,7,9,10,12,13])\n",
    "\n",
    "sparse_csr = sparse.csr_matrix((data2, col_pos, row_pos_ind))\n",
    "\n",
    "print(sparse_coo.toarray())\n",
    "print('-'*20)\n",
    "print(sparse_csr.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "reasonable-birth",
   "metadata": {},
   "outputs": [],
   "source": [
    "## example\n",
    "dense3 = np.array([[0,0,1,0,0,5],\n",
    "                   [1,4,0,3,2,5],\n",
    "                   [0,6,0,3,0,0],\n",
    "                   [2,0,0,0,0,0],\n",
    "                   [0,0,0,7,0,8],\n",
    "                   [1,0,0,0,0,0]])\n",
    "\n",
    "coo = sparse.coo_matrix(dense3)\n",
    "csr = sparse.csr_matrix(dense3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cultural-directive",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 2)\t1\n",
      "  (0, 5)\t5\n",
      "  (1, 0)\t1\n",
      "  (1, 1)\t4\n",
      "  (1, 3)\t3\n",
      "  (1, 4)\t2\n",
      "  (1, 5)\t5\n",
      "  (2, 1)\t6\n",
      "  (2, 3)\t3\n",
      "  (3, 0)\t2\n",
      "  (4, 3)\t7\n",
      "  (4, 5)\t8\n",
      "  (5, 0)\t1\n"
     ]
    }
   ],
   "source": [
    "print(coo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "opposite-chick",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 2)\t1\n",
      "  (0, 5)\t5\n",
      "  (1, 0)\t1\n",
      "  (1, 1)\t4\n",
      "  (1, 3)\t3\n",
      "  (1, 4)\t2\n",
      "  (1, 5)\t5\n",
      "  (2, 1)\t6\n",
      "  (2, 3)\t3\n",
      "  (3, 0)\t2\n",
      "  (4, 3)\t7\n",
      "  (4, 5)\t8\n",
      "  (5, 0)\t1\n"
     ]
    }
   ],
   "source": [
    "print(csr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continuing-glossary",
   "metadata": {},
   "source": [
    "## 20newsgroups Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "proper-purple",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'filenames', 'target_names', 'target', 'DESCR'])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "news_data = fetch_20newsgroups(subset='all', random_state=156)\n",
    "\n",
    "news_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "catholic-outdoors",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18846"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(news_data.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cathedral-private",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'From: egreen@east.sun.com (Ed Green - Pixel Cruncher)\\nSubject: Re: Observation re: helmets\\nOrganization: Sun Microsystems, RTP, NC\\nLines: 21\\nDistribution: world\\nReply-To: egreen@east.sun.com\\nNNTP-Posting-Host: laser.east.sun.com\\n\\nIn article 211353@mavenry.altcit.eskimo.com, maven@mavenry.altcit.eskimo.com (Norman Hamer) writes:\\n> \\n> The question for the day is re: passenger helmets, if you don\\'t know for \\n>certain who\\'s gonna ride with you (like say you meet them at a .... church \\n>meeting, yeah, that\\'s the ticket)... What are some guidelines? Should I just \\n>pick up another shoei in my size to have a backup helmet (XL), or should I \\n>maybe get an inexpensive one of a smaller size to accomodate my likely \\n>passenger? \\n\\nIf your primary concern is protecting the passenger in the event of a\\ncrash, have him or her fitted for a helmet that is their size.  If your\\nprimary concern is complying with stupid helmet laws, carry a real big\\nspare (you can put a big or small head in a big helmet, but not in a\\nsmall one).\\n\\n---\\nEd Green, former Ninjaite |I was drinking last night with a biker,\\n  Ed.Green@East.Sun.COM   |and I showed him a picture of you.  I said,\\nDoD #0111  (919)460-8302  |\"Go on, get to know her, you\\'ll like her!\"\\n (The Grateful Dead) -->  |It seemed like the least I could do...\\n\\n'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_data.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "placed-skill",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"From: jlevine@rd.hydro.on.ca (Jody Levine)\\nSubject: Re: insect impacts\\nOrganization: Ontario Hydro - Research Division\\nLines: 64\\n\\nI feel childish.\\n\\nIn article <1ppvds$92a@seven-up.East.Sun.COM> egreen@East.Sun.COM writes:\\n>In article 7290@rd.hydro.on.ca, jlevine@rd.hydro.on.ca (Jody Levine) writes:\\n>>>>\\n>>>>how _do_ the helmetless do it?\\n>>>\\n>>>Um, the same way people do it on \\n>>>horseback\\n>>\\n>>not as fast, and they would probably enjoy eating bugs, anyway\\n>\\n>Every bit as fast as a dirtbike, in the right terrain.  And we eat\\n>flies, thank you.\\n\\nWho mentioned dirtbikes? We're talking highway speeds here. If you go 70mph\\non your dirtbike then feel free to contribute.\\n\\n>>>jeeps\\n>>\\n>>you're *supposed* to keep the windscreen up\\n>\\n>then why does it go down?\\n\\nBecause it wouldn't be a Jeep if it didn't. A friend of mine just bought one\\nand it has more warning stickers than those little 4-wheelers (I guess that's\\nbecuase it's a big 4 wheeler). Anyway, it's written in about ten places that\\nthe windshield should remain up at all times, and it looks like they've made\\nit a pain to put it down anyway, from what he says. To be fair, I do admit\\nthat it would be a similar matter to drive a windscreenless Jeep on the \\nhighway as for bikers. They may participate in this discussion, but they're\\nprobably few and far between, so I maintain that this topic is of interest\\nprimarily to bikers.\\n\\n>>>snow skis\\n>>\\n>>NO BUGS, and most poeple who go fast wear goggles\\n>\\n>So do most helmetless motorcyclists.\\n\\nNotice how Ed picked on the more insignificant (the lower case part) of the \\ntwo parts of the statement. Besides, around here it is quite rare to see \\nbikers wear goggles on the street. It's either full face with shield, or \\nopen face with either nothing or aviator sunglasses. My experience of \\nbicycling with contact lenses and sunglasses says that non-wraparound \\nsunglasses do almost nothing to keep the crap out of ones eyes.\\n\\n>>The question still stands. How do cruiser riders with no or negligible helmets\\n>>stand being on the highway at 75 mph on buggy, summer evenings?\\n>\\n>helmetless != goggleless\\n\\nOk, ok, fine, whatever you say, but lets make some attmept to stick to the\\npoint. I've been out on the road where I had to stop every half hour to clean\\nmy shield there were so many bugs (and my jacket would be a blood-splattered\\nmess) and I'd see guys with shorty helmets, NO GOGGLES, long beards and tight\\nt-shirts merrily cruising along on bikes with no windscreens. Lets be really\\nspecific this time, so that even Ed understands. Does anbody think that \\nsplattering bugs with one's face is fun, or are there other reasons to do it?\\nImage? Laziness? To make a point about freedom of bug splattering?\\n\\nI've        bike                      like       | Jody Levine  DoD #275 kV\\n     got a       you can        if you      -PF  | Jody.P.Levine@hydro.on.ca\\n                         ride it                 | Toronto, Ontario, Canada\\n\""
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_data.data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "outstanding-virtue",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18846\n",
      "18846\n",
      "20\n",
      "18846\n",
      "9442\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "for key in news_data.keys():\n",
    "    print(len(news_data[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "american-norway",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism',\n",
       " 'comp.graphics',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'comp.windows.x',\n",
       " 'misc.forsale',\n",
       " 'rec.autos',\n",
       " 'rec.motorcycles',\n",
       " 'rec.sport.baseball',\n",
       " 'rec.sport.hockey',\n",
       " 'sci.crypt',\n",
       " 'sci.electronics',\n",
       " 'sci.med',\n",
       " 'sci.space',\n",
       " 'soc.religion.christian',\n",
       " 'talk.politics.guns',\n",
       " 'talk.politics.mideast',\n",
       " 'talk.politics.misc',\n",
       " 'talk.religion.misc']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_data.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "dress-composer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 8,  8, 12, ...,  7,  3,  9])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "acceptable-sunrise",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     799\n",
       "1     973\n",
       "2     985\n",
       "3     982\n",
       "4     963\n",
       "5     988\n",
       "6     975\n",
       "7     990\n",
       "8     996\n",
       "9     994\n",
       "10    999\n",
       "11    991\n",
       "12    984\n",
       "13    990\n",
       "14    987\n",
       "15    997\n",
       "16    910\n",
       "17    940\n",
       "18    775\n",
       "19    628\n",
       "dtype: int64"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(news_data.target).value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "standard-fitting",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: egreen@east.sun.com (Ed Green - Pixel Cruncher)\n",
      "Subject: Re: Observation re: helmets\n",
      "Organization: Sun Microsystems, RTP, NC\n",
      "Lines: 21\n",
      "Distribution: world\n",
      "Reply-To: egreen@east.sun.com\n",
      "NNTP-Posting-Host: laser.east.sun.com\n",
      "\n",
      "In article 211353@mavenry.altcit.eskimo.com, maven@mavenry.altcit.eskimo.com (Norman Hamer) writes:\n",
      "> \n",
      "> The question for the day is re: passenger helmets, if you don't know for \n",
      ">certain who's gonna ride with you (like say you meet them at a .... church \n",
      ">meeting, yeah, that's the ticket)... What are some guidelines? Should I just \n",
      ">pick up another shoei in my size to have a backup helmet (XL), or should I \n",
      ">maybe get an inexpensive one of a smaller size to accomodate my likely \n",
      ">passenger? \n",
      "\n",
      "If your primary concern is protecting the passenger in the event of a\n",
      "crash, have him or her fitted for a helmet that is their size.  If your\n",
      "primary concern is complying with stupid helmet laws, carry a real big\n",
      "spare (you can put a big or small head in a big helmet, but not in a\n",
      "small one).\n",
      "\n",
      "---\n",
      "Ed Green, former Ninjaite |I was drinking last night with a biker,\n",
      "  Ed.Green@East.Sun.COM   |and I showed him a picture of you.  I said,\n",
      "DoD #0111  (919)460-8302  |\"Go on, get to know her, you'll like her!\"\n",
      " (The Grateful Dead) -->  |It seemed like the least I could do...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(news_data.data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "packed-edward",
   "metadata": {},
   "source": [
    "### train, test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "opposed-google",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data size: 11314, Test data size: 7532\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "train_news = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'), random_state=156)\n",
    "\n",
    "train_x = train_news.data\n",
    "train_y = train_news.target\n",
    "\n",
    "test_news = fetch_20newsgroups(subset='test', remove=('headers', 'footers', 'quotes'), random_state=156)\n",
    "\n",
    "test_x = test_news.data\n",
    "test_y = test_news.target\n",
    "print(f\"Train data size: {len(train_x)}, Test data size: {len(test_x)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "silent-singapore",
   "metadata": {},
   "source": [
    "## CountVectorizer 이용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "soviet-homeless",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11314, 101631)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cnt_vect = CountVectorizer()\n",
    "cnt_vect.fit(train_x)\n",
    "train_x_cnt_vect = cnt_vect.transform(train_x)\n",
    "\n",
    "# test data도 train data와 같은 feature를 가지도록 transform\n",
    "test_x_cnt_vect = cnt_vect.transform(test_x)\n",
    "\n",
    "print(train_x_cnt_vect.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rough-start",
   "metadata": {},
   "source": [
    "train data를 CountVectorizer를 이용하여 feature extraction을 진행한 결과 11314개 문서에서 feature라고 할 수 있는 단어가 101631개 만들어졌음을 확인할 수 있다. 이를 이용하여 로지스틱 회귀를 적용하여 뉴스그룹에 대한 classification을 할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ancient-aerospace",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x101631 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 55 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x_cnt_vect[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "beneficial-bidding",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 2223)\t1\n",
      "  (0, 16251)\t1\n",
      "  (0, 16406)\t1\n",
      "  (0, 17936)\t1\n",
      "  (0, 18903)\t1\n",
      "  (0, 19756)\t1\n",
      "  (0, 20123)\t1\n",
      "  (0, 21987)\t1\n",
      "  (0, 23663)\t2\n",
      "  (0, 23790)\t1\n",
      "  (0, 24444)\t1\n",
      "  (0, 25370)\t1\n",
      "  (0, 25590)\t3\n",
      "  (0, 26271)\t3\n",
      "  (0, 26277)\t1\n",
      "  (0, 26992)\t1\n",
      "  (0, 28805)\t1\n",
      "  (0, 31939)\t1\n",
      "  (0, 33551)\t1\n",
      "  (0, 33799)\t1\n",
      "  (0, 35147)\t2\n",
      "  (0, 38824)\t1\n",
      "  (0, 41715)\t1\n",
      "  (0, 43217)\t2\n",
      "  (0, 43961)\t2\n",
      "  :\t:\n",
      "  (11313, 82046)\t1\n",
      "  (11313, 82393)\t1\n",
      "  (11313, 83426)\t1\n",
      "  (11313, 84598)\t1\n",
      "  (11313, 84995)\t1\n",
      "  (11313, 86607)\t1\n",
      "  (11313, 88273)\t1\n",
      "  (11313, 88532)\t5\n",
      "  (11313, 88694)\t1\n",
      "  (11313, 88755)\t1\n",
      "  (11313, 88767)\t2\n",
      "  (11313, 89360)\t5\n",
      "  (11313, 90780)\t1\n",
      "  (11313, 92629)\t2\n",
      "  (11313, 92875)\t1\n",
      "  (11313, 93870)\t1\n",
      "  (11313, 96138)\t1\n",
      "  (11313, 96429)\t3\n",
      "  (11313, 96433)\t1\n",
      "  (11313, 96683)\t1\n",
      "  (11313, 96917)\t1\n",
      "  (11313, 96940)\t1\n",
      "  (11313, 97181)\t1\n",
      "  (11313, 99908)\t1\n",
      "  (11313, 100208)\t1\n"
     ]
    }
   ],
   "source": [
    "print(train_x_cnt_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "compatible-frontier",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'decode'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-68-0a969382703b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mclassifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x_cnt_vect\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_x_cnt_vect\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"CountVectorized Logistic Regression Accuracy: {accuracy_score(test_y, pred):.3f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1405\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1406\u001b[0m             \u001b[0mprefer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'processes'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         fold_coefs_ = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,\n\u001b[0m\u001b[1;32m   1408\u001b[0m                                \u001b[0;34m**\u001b[0m\u001b[0m_joblib_parallel_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprefer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m             path_func(X, y, pos_class=class_, Cs=[C_],\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1039\u001b[0m             \u001b[0;31m# remaining jobs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1040\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1041\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1042\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1043\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    857\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    775\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    776\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 777\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    778\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    570\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 572\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0;31m# change the default number of processes to -1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m             return [func(*args, **kwargs)\n\u001b[0m\u001b[1;32m    263\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0;31m# change the default number of processes to -1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m             return [func(*args, **kwargs)\n\u001b[0m\u001b[1;32m    263\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\u001b[0m in \u001b[0;36m_logistic_regression_path\u001b[0;34m(X, y, pos_class, Cs, fit_intercept, max_iter, tol, verbose, solver, coef, class_weight, dual, penalty, intercept_scaling, multi_class, random_state, check_input, max_squared_sum, sample_weight, l1_ratio)\u001b[0m\n\u001b[1;32m    760\u001b[0m                 \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"iprint\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0miprint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"gtol\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"maxiter\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m             )\n\u001b[0;32m--> 762\u001b[0;31m             n_iter_i = _check_optimize_result(\n\u001b[0m\u001b[1;32m    763\u001b[0m                 \u001b[0msolver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt_res\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m                 extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/optimize.py\u001b[0m in \u001b[0;36m_check_optimize_result\u001b[0;34m(solver, result, max_iter, extra_warning_msg)\u001b[0m\n\u001b[1;32m    241\u001b[0m                 \u001b[0;34m\"    https://scikit-learn.org/stable/modules/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m                 \u001b[0;34m\"preprocessing.html\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m             ).format(solver, result.status, result.message.decode(\"latin1\"))\n\u001b[0m\u001b[1;32m    244\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_warning_msg\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m                 \u001b[0mwarning_msg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mextra_warning_msg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'decode'"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(train_x_cnt_vect, train_y)\n",
    "pred = classifier.predict(test_x_cnt_vect)\n",
    "print(f\"CountVectorized Logistic Regression Accuracy: {accuracy_score(test_y, pred):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "intended-warrior",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Logistic Regression Accuracy: 0.674\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vect = TfidfVectorizer()\n",
    "tfidf_vect.fit(train_x)\n",
    "train_x_tfidf_vect = tfidf_vect.transform(train_x)\n",
    "test_x_tfidf_vect = tfidf_vect.transform(test_x)\n",
    "\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(train_x_tfidf_vect, train_y)\n",
    "pred = classifier.predict(test_x_tfidf_vect)\n",
    "print(f\"TF-IDF Logistic Regression Accuracy: {accuracy_score(test_y, pred):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bored-brass",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
